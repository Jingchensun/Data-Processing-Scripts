{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Name:\n",
    "# Your Studen ID:\n",
    "#################################\n",
    "# Coding Work1: K-means Implementation \n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_wine\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class KMeans:\n",
    "    def __init__(self, n_clusters=3, max_iters=100):\n",
    "        \"\"\"\n",
    "        Initialize KMeans clustering algorithm.\n",
    "\n",
    "        Parameters:\n",
    "        - n_clusters (int): Number of clusters.\n",
    "        - max_iters (int): Maximum number of iterations.\n",
    "        \"\"\"\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iters = max_iters\n",
    "\n",
    "    def fit(self, X): #### Implement this function, this function worth 20 points ####\n",
    "        \"\"\"\n",
    "        Fit KMeans to the data.\n",
    "        Parameters:\n",
    "        - X (array-like): Input data.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "def inertia(X, labels, centroids): #### Implement this function, this function worth 10 points ####\n",
    "    \"\"\"\n",
    "    Calculate the inertia of KMeans clustering.\n",
    "\n",
    "    Parameters:\n",
    "    - X (array-like): Input data.\n",
    "    - labels (array-like): Cluster labels.\n",
    "    - centroids (array-like): Cluster centroids.\n",
    "\n",
    "    Returns:\n",
    "    - float: Inertia value.\n",
    "    \"\"\"\n",
    "    return \n",
    "\n",
    "def visualize_clusters(X, labels, centroids):\n",
    "    \"\"\"\n",
    "    Visualize KMeans clustering results.\n",
    "\n",
    "    Parameters:\n",
    "    - X (array-like): Input data.\n",
    "    - labels (array-like): Cluster labels.\n",
    "    - centroids (array-like): Cluster centroids.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=labels)\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1], marker='x', color='red')\n",
    "    plt.title(\"K-means Clustering\")\n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    # Load the Wine dataset\n",
    "    wine = load_wine()\n",
    "    # Use only the first two features for visualization\n",
    "    X = wine.data[:, :2]  \n",
    "\n",
    "    # Perform KMeans clustering\n",
    "    kmeans = KMeans(n_clusters=3)\n",
    "    kmeans.fit(X)\n",
    "\n",
    "    # Compute inertia\n",
    "    print(\"Inertia:\", inertia(X, kmeans.labels_, kmeans.cluster_centers_))\n",
    "\n",
    "    # Visualize clusters\n",
    "    visualize_clusters(X, kmeans.labels_, kmeans.cluster_centers_)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "# Coding Work2: How to choose K in K-means\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_inertia_vs_k(X, max_k=10): #### Implement this function, this function worth 10 points ####\n",
    "    \"\"\"\n",
    "    Plot the inertia versus the number of clusters.\n",
    "\n",
    "    Parameters:\n",
    "    - X (array-like): Input data.\n",
    "    - max_k (int): Maximum number of clusters to consider. Default is 10.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "\n",
    "def plot_silhouette_vs_k(X, max_k=10): #### Implement this function, this function worth 10 points ####\n",
    "    \"\"\"\n",
    "    Plot the silhouette score versus the number of clusters.\n",
    "\n",
    "    Parameters:\n",
    "    - X (array-like): Input data.\n",
    "    - max_k (int): Maximum number of clusters to consider. Default is 10.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "\n",
    "def main():\n",
    "    # Load the Wine dataset\n",
    "    wine = load_wine()\n",
    "    X = wine.data[:, :2]  # Use only the first two features for visualization\n",
    "\n",
    "    # Plot Inertia vs Number of Clusters\n",
    "    plot_inertia_vs_k(X)\n",
    "\n",
    "    # Plot Silhouette Score vs Number of Clusters\n",
    "    plot_silhouette_vs_k(X)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "# Coding Work3: PCA Implementation\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "class PCA:\n",
    "    def __init__(self, n_components):\n",
    "        \"\"\"\n",
    "        Initialize PCA object.\n",
    "\n",
    "        Parameters:\n",
    "        - n_components (int): Number of principal components to keep.\n",
    "\n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        self.n_components = n_components\n",
    "\n",
    "    def fit_transform(self, X): #### Implement this function, this function worth 20 points ####\n",
    "        \"\"\"\n",
    "        Fit PCA to the data and transform it into the principal components.\n",
    "\n",
    "        Parameters:\n",
    "        - X (array-like): Input data.\n",
    "\n",
    "        Returns:\n",
    "        - X_pca (array-like): Transformed data in the principal component space.\n",
    "        \"\"\"\n",
    "\n",
    "    def visualize_features_vs_components(self, X, X_std, X_pca):\n",
    "        \"\"\"\n",
    "        Visualize original features vs principal components.\n",
    "\n",
    "        Parameters:\n",
    "        - X (array-like): Original input data.\n",
    "        - X_std (array-like): Standardized input data.\n",
    "        - X_pca (array-like): Transformed data in the principal component space.\n",
    "\n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "\n",
    "        # Plot original features\n",
    "        for i in range(X.shape[1]):\n",
    "            axes[0].scatter(np.arange(X.shape[0]), X_std[:, i], label=f'Feature {i+1}')\n",
    "\n",
    "        # Plot principal components\n",
    "        for i in range(X_pca.shape[1]):\n",
    "            axes[1].scatter(np.arange(X.shape[0]), X_pca[:, i], label=f'PC {i+1}')\n",
    "\n",
    "        axes[0].set_title('Original Features')\n",
    "        axes[1].set_title('Principal Components')\n",
    "        axes[0].set_xlabel('Sample Index')\n",
    "        axes[1].set_xlabel('Sample Index')\n",
    "        axes[0].set_ylabel('Standardized Value')\n",
    "        axes[1].set_ylabel('Transformed Value')\n",
    "        axes[0].legend()\n",
    "        axes[1].legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_explained_variance_vs_components(self, X): #### Implement this function, this function worth 10 points ####\n",
    "        \"\"\"\n",
    "        Plot explained variance ratio versus number of components.\n",
    "\n",
    "        Parameters:\n",
    "        - X (array-like): Input data.\n",
    "\n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load Wine dataset\n",
    "    wine = load_wine()\n",
    "    X = wine.data\n",
    "\n",
    "    # Perform PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "\n",
    "    # Print shapes of original and transformed data\n",
    "    print(\"Original Shape:\", X.shape)\n",
    "    print(\"Transformed Shape:\", X_pca.shape)\n",
    "\n",
    "    # Plot explained variance ratio versus number of components\n",
    "    pca.plot_explained_variance_vs_components(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "# Coding Work4: NaiveBayes Implementation\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "class NaiveBayes: #### Implement this class, this function worth 20 points ####\n",
    "    def fit(self, X, y): \n",
    "        \"\"\"\n",
    "        Fit the Naive Bayes classifier to the training data.\n",
    "\n",
    "        Parameters:\n",
    "        - X (array-like): Training data, where each row represents a sample and each column represents a feature.\n",
    "        - y (array-like): Target values.\n",
    "\n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the class labels for the input data.\n",
    "\n",
    "        Parameters:\n",
    "        - X (array-like): Input data, where each row represents a sample and each column represents a feature.\n",
    "\n",
    "        Returns:\n",
    "        - array-like: Predicted class labels.\n",
    "        \"\"\"\n",
    "\n",
    "    def pdf(self, X, mean, std):\n",
    "        \"\"\"\n",
    "        Compute the probability density function (PDF) of the Gaussian distribution.\n",
    "\n",
    "        Parameters:\n",
    "        - X (array-like): Input data, where each row represents a sample and each column represents a feature.\n",
    "        - mean (array-like): Mean of the Gaussian distribution.\n",
    "        - std (array-like): Standard deviation of the Gaussian distribution.\n",
    "\n",
    "        Returns:\n",
    "        - array-like: PDF values.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "def plot_decision_boundary(X, y, classifier, title):\n",
    "    \"\"\"\n",
    "    Plot the decision boundary of a classifier along with the data points.\n",
    "\n",
    "    Parameters:\n",
    "    - X (array-like): Input data.\n",
    "    - y (array-like): Target labels.\n",
    "    - classifier: Trained classifier object with a predict method.\n",
    "    - title (str): Title of the plot.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "    cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "    # Determine plot limits\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "\n",
    "    # Generate mesh grid\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                         np.arange(y_min, y_max, 0.02))\n",
    "\n",
    "    # Predict class labels for each point in the mesh grid\n",
    "    Z = classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # Plot decision boundary\n",
    "    plt.figure()\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "    # Plot data points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,\n",
    "                edgecolor='k', s=20)\n",
    "    \n",
    "    # Set plot limits\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "\n",
    "    # Set plot title and labels\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    \n",
    "    # Show plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Load the Wine dataset\n",
    "    wine = load_wine()\n",
    "    X = wine.data[:, :2]  # Use only the first two features for visualization\n",
    "    y = wine.target\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create and fit the Naive Bayes classifier\n",
    "    nb_classifier = NaiveBayes()\n",
    "    nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Predict\n",
    "    y_pred = nb_classifier.predict(X_test)\n",
    "\n",
    "    # Compute accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy of Naive Bayes classifier:\", accuracy)\n",
    "\n",
    "    # Plot decision boundary\n",
    "    plot_decision_boundary(X_train, y_train, nb_classifier, 'Naive Bayes Decision Boundary (Training Set)')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clipood",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
